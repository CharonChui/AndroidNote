大模型简介
---

### 人工智能

人工智能(AI, Artificial Inteligence)通常用于描述致力于执行接近人类智能任务(如语音识别、语音翻译和视觉感知)的计算机系统。它是指软件产生的智能，而不是人类的智能。   

目标是研究、设计和构建具备智能、学习、推理和行动能力的计算机和机器。    

例如: 2016年Alphago击败李世石围棋。     

### 生成式人工智能

GAI (Generative Aritificial Interligence)

目标是让机器能够产生复杂有结构的物件。 例如chatGPT、deepseek。

### 机器学习

ML(Machine Learning)

提到机器学习，追根溯源，我们需要先知道什么是“学习”。著名学者、1975年图灵奖获得者、1978年诺贝尔经济学奖获得者，赫伯特·西蒙（Herbert Simon）教授曾对“学习”下过一个定义：如果一个系统，能够通过执行某个过程，就此改进它的性能，那么这个过程就是学习。
在西蒙看来，学习的核心目的就是改善性能。其实对于人而言，这个定义也是适用的。如果我们仅仅进行低层次的重复性学习，而没有达到认知升级的目的，那么即使表面看起来非常勤奋，其实也仅仅是一个“伪学习者”，因为我们的性能并没有得到改善。
西蒙认为，对于计算机系统而言，通过运用数据及某种特定的方法（比如统计方法或推理方法）来提升机器系统的性能，就是机器学习（Machine Learning，简称ML）。

所谓机器学习，在形式上可近似等同于，在数据对象中通过统计或推理的方法，寻找一个有关特定输入和预期输出的功能函数f


所有的监督学习（Supervised Learning），基本上都是分类（Classification）的代名词。它从有标签的训练数据中学习模型，然后对某个给定的新数据利用模型预测它的标签。
比如，小时候父母告诉我们某个动物是猫、狗或猪，然后在我们的大脑里就会形成或猫或狗或猪的印象（相当于模型构建），当面前来了一只“新”小狗时，如果你能叫出“这是一只小狗”，那么恭喜你，标签分类成功！
但如果你回答说“这是一头小猪”，这时你的父母就会纠正你的偏差：“乖，不对，这是一只小狗。”这样一来二去地进行训练，不断更新你大脑的认知体系，当下次再遇到这类新的猫、狗、猪时，你就会在很大概率上做出正确的“预测”分类。
监督学习，就是先利用有标签的训练数据学习得到一个模型，然后使用这个模型对新样本进行预测。在本质上，监督学习的目标在于，构建一个由输入到输出的映射，该映射用模型来表示。
### 深度学习

DL(Deep Learning): 深度学习算法试图模拟人类大脑的工作方式，其灵感来源于神经生物学，它通过对大量数据的学习，自动提取出数据的高层次特征和模式，从而实现图像识别、语音识别、自然
语言处理等任务。 

按照架构的不同，神经网络可以分为:   

- 卷积神经网络(CNNS)
- 循环神经网络(RNNS)
- Transformer网络等

同样是区分不同水果，这次你带孩子去了超市，那里有各种不同的水果，你没有解释每种水果的特点，只是给孩子指出了哪些是苹果、哪些是香蕉，他通过观察和比较，慢慢学会了辨认各种水果。 

在这个过程中，孩子的大脑(在这里比喻为深度学习模型)自动从复杂的视觉、嗅觉等信号中提取层次化的特征。比如圆形、条纹、颜色深浅、气味等，从而达到识别水果的目的。  


### LoRA

低秩自适应(Low-Rank Adaptation，LoRA)是一种针对大型预训练语言模型（如GPT-3）的微调技术。LoRA的核心思想是通过对模型权重进行低秩更新，以实现有效的参数适应，而无须重新训练模型的全部参数。这种技术在保留预训练模型的泛化能力的同时，允许模型快速适应特定任务。


### 大语言模型

LLM(Large Language Model)


![Image](https://raw.githubusercontent.com/CharonChui/Pictures/master/ai_relation..png?raw=true)        



### 多模态

提及LLM时，我们通常不会第一时间联想到多模态。毕竟，LLM本质上是语言模型。但我们很快会发现，若能处理文本之外的数据类型，其应用价值将大幅提升。例如，当语言模型可“看到”图像并回答相关问题时，其效用将显著增强。这种能够处理文本与图像（每种数据类型称为一种模态）的模型，即被称为多模态(multimodal)模型

能处理多种数据模态（如图像、音频、视频或传感器数据）的模型称为多模态模型。模型接收某种模态作为输入，但不一定能生成对应模态的输出

### 什么是大模型

大语言模型(large language model，LLM)[插图]已经对世界产生了深远的影响。通过使机器更好地理解和生成人类语言，LLM在人工智能(artificial inteligence，AI)领域开创了新的可能性，并影响了众多行业。


大模型，一般也称为"大预言模型"， 是一种基于深度学习技术训练出来的人工智能系统，主要用于处理和生成人类语言。     

模型规模: 通常包含数十亿到数千亿个参数，这些参数就像是模型的"大脑神经元"。     
训练数据: 使用海量文本数据进行训练，包括书籍、文章、网页等各种形式的文字内容。

在深度学习和像GPT这样的大语言模型中，“参数”指的是模型的可训练权重。这些权重本质上是模型的内部变量，在训练过程中通过调整和优化来最小化特定的损失函数。


### 模型蒸馏

大模型的运行需要极高的硬件资源，通常都是服务器集群并挂载数量众多的GPU(显卡)。
为了满足低性能设备的运行，可以对大模型进行蒸馏。   例如deepseek r1的 7b模型、70b模型。 

模型蒸馏: 对模型进行简化，在尽量保证模型性能的前提下，尽可能减少对硬件资源的需求。  

简单理解就是假设你有一个超强的老师(大模型)，他能讲解很复杂的知识。
你想把这些知识传授给一群学生(小模型)。为了让学生能在不需要过多时间和资源的情况下，快速掌握老师的知识。你可以通过“蒸馏的方式”，让学生只学习精华、最重要的部分。
这样，学生虽然没有老师那么强大，单依然能做出优秀表现。   




### 大模型训练

大模型整体的训练主要分为三个阶段:    

- 预训练: 注入领域知识     
- SFT(监督微调):      
- RLHF(基于人类反馈的强化学习)     



### 模型部署


#### ollama

[ollama](https://ollama.com/)是一款旨在简化大型语言模型本地部署和运行过程的开源软件。   


- 本地部署(ollama)
    - 后端和模型部署(例如ollama本地部署千问模型)
    - 前端部署(交互界面，例如chatbox ai、Stremlit)
    - 用户使用

#### LangChain


[LangChain](https://www.langchain.com/)是一个用于构建和管理基于语言模型(Language Models，LM)的应用程序框架。      

它提供了一系列工具和组件，帮助开发者更高效的构建、训练、部署和管理语言模型应用。      

LangChain的设计目标是简化语言模型的使用过程，使其更加容易被集成到各种应用场景中。      
它本质上是一个Python框架

LangChain的名称源自其核心方法之一——链(chain)。虽然我们可以独立运行LLM，但链的真正威力在与其他组件协同工作，甚至在多条链相互配合时才能充分展现。这种架构不仅能拓展LLM的能力，还能实现链与链之间的无缝衔接。



LangChain主要组件:   

- Prompts: 提示，包括提示管理、提示优化和提示序列化
- Models: 模型，各种类型的模型和模型集成，比如gpt-4、deepseek
- Memory: 记忆，用来保存和模型交互时的上下文状态
- Indexes: 索引，用来结构化文档，以便和模型交互
- Chains: 链，一系列对各种组件的调用
- Agents: 代理，决定模型采取哪些行动，执行并且观察流程，直到完成为止



- 云端部署(LangChain)
    - 后端部署(例如阿里云百炼平台部署千问模型)
    - 前端部署(例如Stremlit)
    - 用户使用



模型部署总结:   

Ollama是本地模型运行的入门工具。     
LangChain是覆盖入门 + 进阶的全功能框架。    

除了Ollama的功能外，LangChain额外多出了:   

- 记忆能力: 基于ConversationBufferMemory等组件完成多轮对话历史记录    
- 文档对接能力: 可实现PDF/Word等本地文档对打，构建私有知识库
- 工具调用与智能体能力: 通过Tool、Agent组件让模型自动调用工具
- 流程编排能力: SequentialChain、LCEL等实现任务自动化与并行模型

以及，针对云平台的API调用支持(如阿里云的通义千问百炼平台、腾讯云的混元等)    



## 端模型

不是所有模型都奔着“更大更强”去才有价值。    

相反，像Qwen3-0.6b这种小模型才是真正能在实际场景中跑起来、用得起的模型。   

Qwen3-0.6b的表现:   

- 推理速度快、延迟低：典型场景延迟在几十毫秒，适合边缘设备部署。  
- 资源占用小: 内存带宽压力低，功耗控制出色，支持长时间稳定运行。 
- 兼容性强: 广泛适用于query改写、文本匹配、语义检索等任务。   

 
很多人认为Qwen3-0.6b不如7b、13b强大，因此没用。但现实是，在高并发、低延迟、受限资源的实际业务场景中，大模型往往根本上不了场。   

总结: 轻、小、稳，才是能落地的关键。   

举个例子:   

普通小货车的运载能力相比较那些拉上十几吨、几十吨的大货车相差太多，难道它就没用用处吗？      

当然不是，因为它的定位就是面对相对狭小的场景、相对少的货物做出更敏捷的选择。    

Qwen3-0.6b的定位，从来也不是生成长文本或复杂推理，而是在多模态系统中提供“一点点启发性的信号”，用于放大系统整体效能。      

这“一点点信号”在推荐、搜索、识别系统重，往往正是决定点击、转化、留存的关键。   


我们选择部署Qwen3-0.6b，不是"凑合"，而是精准选择--它足够快、足够轻、足够稳，也足够使用。     

在生产环境中，能跑、能稳、能提效的模型，才是真正有价值的模型。   



### 大模型的工作流程


大多数人与语言模型交互的方式是通过网页平台，它提供用户与语言模型之间的聊天界面。你可能会注意到，模型并不是一次性生成所有输出，而是一次生成一个词元。
词元不仅是模型的输出单位，也是模型查看输入的方式。发送给模型的提示词首先被分解成词元。



对计算机来说，语言是一个复杂的概念。   


文本本质上是非结构化的，当用0和1(单个字符)表示时就会失去其含义。因此，在语言人工智能的发展历史中，人们一直非常关注如何以结构化的方式标识语言，使计算机能够更容易的使用。    

语言人工智能历史始于一种名为词袋(bag of words)的技术，这是一种表示非结构化文本的方法。  

词袋模式的工作原理如下: 假设我们有两个句子需要创建数值表示。

词袋模型的第一步是分词(tokenization)，即将句子拆分为单个词或子词(词元(token))。   

最常见的分词方法是通过空格分割来把句子分割成词。      

然而，这种方法也有其缺点，因为某些语言(如汉语)的词之间没有空格。
在分词之后，我们将每个句子中所有不同的词组合起来，创建一个可用于表示句子的词表(vocabulary)。
使用词表，我们只需计算每个句子中词出现的次数，就创建了一个词袋。       
因此，词袋模型旨在以数字形式创建文本的表示(representation)，也称为向量或向量的标识。我们将这类模型称为表示模型(representation model)    



![Image](https://raw.githubusercontent.com/CharonChui/Pictures/master/respresentation_1.png?raw=true)        

通过计算单个词出现的次数创建词袋，这些值被称为向量表示。


词袋虽然是一种优雅的方法，但存在一个明显的缺陷。它仅仅把语言视为一个几乎字面意义上的"词袋"，而忽略了文本的语音特性和含义。
这是因为词袋模型就像把一段话拆成一堆词，装进袋子里，只数数量，不看顺序，不懂意思，不管谁和谁有关系。这让它在理解句子真正含义时，显的很呆。     
比如狗追猫和猫追狗。 词袋模型看不出来到底谁追谁。    


word2vec(词向量)于2013年发布，是首批成功利用嵌入(embeding)这个概念来捕捉文本含义的技术之一。     

嵌入是数据的向量表示，试图捕捉数据的含义。为此word2vec通过在大量文本数据(如整个维基百科)上训练来学习词的语义表示。   

为了生成这些语义表示，word2vec利用了神经网络(neural network)技术。    

神经网络由处理信息的多层互连节点组成。神经网络可以有多个分层，每个连接有一定的权重，这些权重通常被称为模型的参数。    


![Image](https://raw.githubusercontent.com/CharonChui/Pictures/master/neural_network_1.png?raw=true)        

在训练过程中，word2vec会学习词与词之间的关系，并将这些信息提炼到词嵌入中。如果两个词各自的相邻词集合有更大的交集，它们的词嵌入向量就会更接近，反之亦然。    

词嵌入可以用多种属性来表示一个词的含义。由于嵌入向量的大小是固定的，这些属性需要经过精心选择，以构建用来代表词的“心智表征”的抽象标识。 

![Image](https://raw.githubusercontent.com/CharonChui/Pictures/master/word2vec_2.png?raw=true)        

在实践中，这些属性通常相当抽象，很少与单一实体或人类可识别的概念相关。然而，这些属性组合在一起对计算机来说是有意义的，是将人类语言转换为计算机语言行之有效的方式。     

词嵌入非常有用，因为它使我们能够衡量两个词的语义相似度。   

上面word2vec的训练过程会创建静态的、可下载的词标识。例如，bank这个词无论在什么上下文中使用，都会有相同的词嵌入。   

然而，bank既可以指银行，也可以指河岸。它的含义应该根据上下文而变化，因此它的嵌入也应该根据上下文而变化。   
使用RNN(Recurrent Neural Network，循环神经网络)，可以实现文本编码的一个步骤。这些神经网络的变体可以将序列作为补充输入进行建模。    
为此，这些RNN被用于两个任务:    
- 编码: 也就是表示输入句子
- 解码: 也就是输出句子

该架构中的每个步骤都是自回归(auto - regressvie)的，在生成下一个词时，该架构需要使用所有先前生成的词作为输入。    
然而，这种上下文嵌入方式存在局限性，因为它仅用一个嵌入向量来表示整个输入，使得处理较长的句子变得困难。    

在2014年，研究人员提出了注意力(attention)解决方案，大大改善了原始架构。    
注意力云讯模型关注输入序列中彼此相关(相互注意)的部分，并放大它们的信号，注意力机制通过选择性地聚焦于句子中最关键的词，来突出其重要性。   

### Transformer

在2017年著名论文"Attention is All You Need"首次探讨了注意力机制的真正威力，并提出了一种被称为Transformer的网络架构。    

Transformer完全基于注意力机制，摒弃了此前提到的RNN。与RNN相比，Transformer支持并行训练，这大大加快了训练速度。    
在Transformer中，编码和解码组件相互堆叠。这种架构仍然是自回归的，每个新生成的词都被模型用于生成下一个词。   

![Image](https://raw.githubusercontent.com/CharonChui/Pictures/master/transformer_1.png?raw=true)        

Transformer由堆叠的编码器和解码器块组合而成，输入依次轮流经每个编码器和解码器. 

要理解Transformer LLM的行为，最常见的方式是将其视为一个接收文本输入并生成响应文本的软件系统。 

通过分词后大模型会生成一个个的新词元的概率列表，然后从生成的词元列表中选择某一个词元的过程就叫做解码策略。 

每次都选择概率分数最高的词元的策略被称为贪心解码。这就是在LLM中将温度(temperature)参数设为零时会发生的情况。



![Image](https://raw.githubusercontent.com/CharonChui/Pictures/master/transformer_2.png?raw=true)        

· Transformer LLM 每次生成一个词元。     
· 生成的词元会被追加到提示词中，然后，这个更新后的提示词会再次被输入模型进行下一次前向传播，以生成下一个词元。    
· Transformer LLM的三个主要组件是分词器、一系列Transformer块和语言建模头。    
· 分词器包含模型的词元词表。模型中包含与这些词元相关联的词元嵌入。将文本分解成词元，然后使用这些词元的嵌入向量，是词元生成过程的第一步。    
· 前向传播会依次经过所有阶段。    
· 在处理接近尾声时，语言建模头会对下一个可能的词元进行概率评分。解码策略决定了在这一生成步骤中选择哪个实际词元作为输出（有时是概率最高的下一个词元，但并非总是如此）。    
· Transformer表现出色的原因之一是它能够并行处理词元。每个输入词元都流入其独立的计算流（也称为处理路径）。这些流的数量就是模型的“上下文长度”，代表模型可以处理的最大词元数量。    
· 由于Transformer LLM通过循环来一次生成一个词元的文本，因此缓存每个步骤的处理结果是一种很好的策略，这样可以避免重复处理工作（这些结果以各种矩阵的形式存储在层中）。    
· 大部分处理发生在Transformer块中。这些块由两个组件组成，其中一个是前馈神经网络，它能够存储信息，并根据训练数据进行预测和插值。    
· Transformer块的另一个主要组件是自注意力。自注意力整合了上下文信息，使模型能够更好地捕捉语言的细微差别。    
· 注意力过程分为两个主要步骤：相关性评分；信息组合。    
· Transformer的自注意力层并行执行多个注意力操作，每个操作都发生在注意力头内，它们的输出被聚合成自注意力层的输出。    
· 通过在所有注意力头或一组注意力头（分组查询注意力）之间共享键矩阵和值矩阵，可以加速注意力计算。    
· Flash Attention等方法通过优化在GPU不同显存系统上的操作方式来加速注意力计算。    







##### 1. 分词化与词表映射

分词化(Tokenization)是自然语言处理(NLP)中的重要概念，它是将段落和句子分割成更小的分词(Token)的过程。    

举一个实际的例子，以下是一个英文句子:    

I want to study LLM.   

为了让机器理解这个句子，对字符串执行分词化，将其分解为独立的单元。使用分词化，我们会得到这样的结果:     

`["I", "want", "to", "study", "LLM", "."]`

将一个句子分解成更小的、独立的部分可以帮助计算机理解句子的各个部分，以及它们在上下文中的作用，这对于进行大量上下文的分析尤其重要。   

每一个分词(token)都会通过预先设置好的词表，映射为一个tokenid，这是token的身份证，这样一句话最终会被表示为一个元素为tokenid的列表，供给计算机进行下一步处理。



##### 2. 文本生成过程


大模型的工作概括来说是根据给定的文本预测下一个token。 

对于我们来说，看似像在对大模型提问，但实际上是给了大模型一串提示文本，让它可以对后续的文本进行推理。   

大模型的推理过程不是一步到位的，当大模型进行推理时，它会基于现有的token，根据概率最大原则预测出下一个最有可能的token，然后将该预测的token加入到输入序列中，并将更新
后的输入序列继续输入大模型预测下一个token。
这个概率则是我们用很多的文本，分词为token以后训练大模型得来的。   



### LLM


LLM大模型训练流程:    
![Image](https://raw.githubusercontent.com/CharonChui/Pictures/master/llm_training_process.png?raw=true)        





### 提示工程

提示工程(Prompt Engineering)是一项通过优化提示词(Prompt)和生成策略，从而获得更好的模型返回结果的工程技术。    

提示词(Prompt)  --> 大语言模型(LLM) -->  返回结果(Completion)

- 好的Prompt需要不断优化
- 说清楚自己到底想要什么，要具体
- 不要让机器去猜测太多，为了不让机器去猜测，我们就需要告诉细节
- 提示工程有一些技巧，灵活掌握，事半功倍。   


一个包含多个组件的复杂提示词示例:     
![Image](https://raw.githubusercontent.com/CharonChui/Pictures/master/prompt_engie_1.png?raw=true)        



### GPU

计算资源通常是指系统中可用的GPU（graphics processing unit，图形处理单元，通常称显卡）资源。      

强大的GPU可以加速LLM的训练和使用。在选择GPU时，一个重要的因素是可用的VRAM（video random access memory，视频随机存储器，通常称显存）容量，即GPU上可用的内存量。实践中，显存越大越好。原因是如果没有足够的显存，某些模型根本无法使用。



### Agent


Agent  = LLM + memory + planning skills + tool use 

目前我们构建的系统均按照预设流程执行操作。LLM最具突破性的发展方向在于其自主决策能力。这类能够自主规划行动及其序列的系统被称为智能体(agent)，其核心在于利用语言模型自主制定行动决策。


使用语言模型的一个重要步骤是选择模型。查找和下载LLM的一个重要网站是Hugging Face。Hugging Face是著名的Transformers软件包背后的组织，多年来一直推动着语言模型的整体发展。正如其名称所示，该软件包是建立在我们在此前提到过的transformers框架之上的。




